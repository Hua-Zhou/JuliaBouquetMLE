{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\ba}{\\boldsymbol{a}}\n",
    "\\newcommand{\\bb}{\\boldsymbol{b}}\n",
    "\\newcommand{\\bc}{\\boldsymbol{c}}\n",
    "\\newcommand{\\bd}{\\boldsymbol{d}}\n",
    "\\newcommand{\\be}{\\boldsymbol{e}}\n",
    "\\newcommand{\\bff}{\\boldsymbol{f}}\n",
    "\\newcommand{\\bg}{\\boldsymbol{g}}\n",
    "\\newcommand{\\bh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\bi}{\\boldsymbol{i}}\n",
    "\\newcommand{\\bj}{\\boldsymbol{j}}\n",
    "\\newcommand{\\bk}{\\boldsymbol{k}}\n",
    "\\newcommand{\\bl}{\\boldsymbol{l}}\n",
    "\\newcommand{\\bm}{\\boldsymbol{m}}\n",
    "\\newcommand{\\bn}{\\boldsymbol{n}}\n",
    "\\newcommand{\\bo}{\\boldsymbol{o}}\n",
    "\\newcommand{\\bp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\bq}{\\boldsymbol{q}}\n",
    "\\newcommand{\\br}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bs}{\\boldsymbol{s}}\n",
    "\\newcommand{\\bt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\bu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\bv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\bw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\bx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\by}{\\boldsymbol{y}}\n",
    "\\newcommand{\\bz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\bA}{\\boldsymbol{A}}\n",
    "\\newcommand{\\bB}{\\boldsymbol{B}}\n",
    "\\newcommand{\\bC}{\\boldsymbol{C}}\n",
    "\\newcommand{\\bD}{\\boldsymbol{D}}\n",
    "\\newcommand{\\bE}{\\boldsymbol{E}}\n",
    "\\newcommand{\\bF}{\\boldsymbol{F}}\n",
    "\\newcommand{\\bG}{\\boldsymbol{G}}\n",
    "\\newcommand{\\bH}{\\boldsymbol{H}}\n",
    "\\newcommand{\\bI}{\\boldsymbol{I}}\n",
    "\\newcommand{\\bJ}{\\boldsymbol{J}}\n",
    "\\newcommand{\\bK}{\\boldsymbol{K}}\n",
    "\\newcommand{\\bL}{\\boldsymbol{L}}\n",
    "\\newcommand{\\bM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\bN}{\\boldsymbol{N}}\n",
    "\\newcommand{\\bO}{\\boldsymbol{O}}\n",
    "\\newcommand{\\bP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\bQ}{\\boldsymbol{Q}}\n",
    "\\newcommand{\\bR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\bS}{\\boldsymbol{S}}\n",
    "\\newcommand{\\bT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\bU}{\\boldsymbol{U}}\n",
    "\\newcommand{\\bV}{\\boldsymbol{V}}\n",
    "\\newcommand{\\bW}{\\boldsymbol{W}}\n",
    "\\newcommand{\\bX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\bY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\bZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\btheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bXi}{\\boldsymbol{\\Xi}}\n",
    "\\newcommand{\\bPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bOmega}{\\boldsymbol{\\Omega}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=\"5\">Julia's Bouquet of MLE Examples</font></b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kenneth Lange](mailto:klange@ucla.edu), Departments of Biomathematics, Human Genetics, and Statistics, University of California, Los Angeles, CA\n",
    "\n",
    "[Hua Zhou](mailto:huazhou@ucla.edu), Department of Biostatistics, University of California, Los Angeles, CA\n",
    "\n",
    "*The authors gratefully acknowledge the support of USPHS grants GM53275 and HG006139.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=\"4\">Abstract</font></b></center>\n",
    "\n",
    "Classroom expositions of maximum likelihood estimation (MLE) rely on traditional\n",
    "calculus methods to construct analytic solutions. This creates in students\n",
    "a false sense of the ease with which MLE problems can be attacked. In a\n",
    "nod to reality, some teachers will mention and possibly apply\n",
    "Newton's method, Fisher scoring, and the EM algorithm. Although preferable to \n",
    "leaving students in a state of ignorance, such brief expositions ultimately\n",
    "fail to expose the full body of relevant techniques. Some of these\n",
    "techniques extend more readily to big data problems than Newton's\n",
    "method and scoring. The current paper emphasizes block ascent, profile\n",
    "likelihoods, the minorization-maximization (MM) principle, and their creative combination. \n",
    "These themes are put to work in readable\n",
    "Julia code to solve several low dimensional MLE problems. With its\n",
    "rich library of special functions and clear, succinct syntax, Julia is\n",
    "an ideal vehicle for solving exceptional MLE problems. This manuscript is \n",
    "prepared using Jupyter Notebook, which interweaves text, code, and output in \n",
    "a single document. It introduces students to the tools for reproducible \n",
    "research in quantitative research. Its source can be used as a template for \n",
    "students' homework and project reports, or even blogs.\n",
    "\n",
    "**Keywords**: block ascent, profile likelihood, MM principle, convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Textbook discussions of maximum likelihood estimation focus\n",
    "on easy derivations of exact formulas.  When confronted with harder problems, \n",
    "texts may mention Newton's method or Fisher scoring, but students seldom\n",
    "get a strong sense of the analytic and numerical strategies\n",
    "that every serious research statistician must master. The detours\n",
    "through special function theory and optimization are viewed as\n",
    "distractions from the main purpose of presenting principles\n",
    "of inference. While this attitude is understandable, it\n",
    "leaves gaps in students' education and misses the opportunity to build\n",
    "bridges to the analysis of big data sets, where choosing the right\n",
    "method of optimization often spells the difference between success\n",
    "and failure. The purpose of the current article is to present\n",
    "some of these strategies in the simple context of one and\n",
    "two-dimensional problems. Here the key ideas are uncluttered by \n",
    "the sophistication of modern mathematical theories such as the convex calculus.\n",
    "Seeing how these ideas play out in various guises and combinations\n",
    "is bound to benefit students.\n",
    "\n",
    "A secondary aim of this article is to acquaint readers with the\n",
    "new programming language `Julia`. If `R` is the current king of statistical\n",
    "computing, then Julia is the upstart pretender to the throne. Julia\n",
    "can be orders of faster than `R`. From a computer science perspective,\n",
    "it is a better designed language.  Because `Julia` trails badly in relevant\n",
    "packages, `R` will dominate for years to come. Most likely `Julia`\n",
    "will first overtake Matlab, the commercial platform favored by applied\n",
    "mathematicians and engineers. `Julia` has syntax similar to `Matlab`'s and shares most of\n",
    "its linear algebra routines and special functions. Because `Julia`\n",
    "is just-in-time (JIT) complied rather than interpreted, it pays no price for loops.\n",
    "`Julia`'s built-in optimization software and library of special functions\n",
    "makes coding of maximum likelihood estimators straightforward. In addition\n",
    "to these advantages, Julia supports dataframes and parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Maximum Likelihood\n",
    "\n",
    "In this survey we will ignore Newton's method, Fisher scoring,\n",
    "and the EM algorithm, the mainstays of traditional computational\n",
    "statistics.  The first two alternatives scale poorly in \n",
    "high-dimensional problems and require extra programming effort\n",
    "to accommodate constraints. The EM algorithm occupies a somewhat different\n",
    "niche. It is by definition well adapted to constraints and guaranteed\n",
    "to increase the likelihood function. Its drawbacks are its often slow rate of \n",
    "convergence, the intellectual effort expended to identify the right missing \n",
    "data framework, and the calculation of challenging conditional expectations. \n",
    "These disadvantages have prompted mathematical scientists to revisit techniques such\n",
    "as gradient ascent and block ascent that work better in high dimensions and \n",
    "apply to non-differentiable functions.\n",
    "\n",
    "Block ascent and descent are sometimes spectacularly successful. One\n",
    "divides the parameters into an exhaustive set of blocks and updates\n",
    "one block at a time, holding the parameters in the other blocks fixed. \n",
    "Usually the block updates are analytic. Coordinate ascent and descent algorithms\n",
    "update one parameter at a time. Constraints can be imposed, but each should\n",
    "be confined to a single block. The method of profile likelihoods\n",
    "is closely related to block methods. Here the optimal parameters in a given block are\n",
    "found in terms of the remaining parameters and inserted in this form\n",
    "into the overall likelihood. The reduced or profile likelihood is then\n",
    "maximized. \n",
    "\n",
    "The third optimization technique we highlight is the MM\n",
    "algorithm \\cite{HunterLange04MMTutorial,lange2016mmoptimization}. This is a principle \n",
    "for algorithm design that retains\n",
    "the attractive features of the EM algorithm  while jettisoning\n",
    "its dependence on missing data. Every EM algorithm is an MM\n",
    "algorithm. The truth of the converse is unknown, but it is now \n",
    "obvious that the MM perspective generates new algorithms that cannot be \n",
    "derived easily from the EM perspective. The skills in deriving the two\n",
    "kinds of algorithms differ. Derivations of MM algorithms\n",
    "typically rely on inequalities rather than conditional\n",
    "expectations. \n",
    "\n",
    "Suppose $f(\\btheta)$ is an objective function in need of maximization. The MM \n",
    "principle suggests devising a surrogate function $g(\\btheta \\mid \\btheta_n)$\n",
    "anchored at the current iterate $\\btheta_n$. The surrogate\n",
    "should be easier to maximize than the objective and hug the\n",
    "objective as tightly as possible. The surrogate is also required to\n",
    "satisfy the two conditions\n",
    "\\begin{eqnarray*}\n",
    "f(\\btheta_n) = g(\\btheta_n \\mid \\btheta_n) \\quad \\text{and}\n",
    "\\quad f(\\btheta) \\ge g(\\btheta \\mid \\btheta_n) \\quad \\forall \\;\\; \\btheta\n",
    "\\end{eqnarray*}\n",
    "of tangency and domination. If $\\btheta_{n+1}$ is chosen to maximize the\n",
    "surrogate, then the ascent property \n",
    "\\begin{eqnarray*}\n",
    "f(\\btheta_{n+1}) \\ge g(\\btheta_{n+1} \\mid \\btheta_n) \n",
    "\\ge g(\\btheta_{n} \\mid \\btheta_n) = f(\\btheta_n)\n",
    "\\end{eqnarray*}\n",
    "follows. When $f(\\btheta)$ must be minimized rather than maximized, the\n",
    "domination inequality should be reversed, and the surrogate should\n",
    "be minimized. The beauty of the MM principle lies in its generality. One \n",
    "can work piecemeal in minorizing loglikelihoods and \n",
    "forgot about missing data. Constraints must be observed in\n",
    "optimizing the surrogate.\n",
    "\n",
    "Jensen's inequality is instrumental in majorizing composite functions \n",
    "$f[\\sum_i u_i(\\btheta)]$, where $f(u)$ is convex and each summand $u_i(\\btheta)$ is a positive \n",
    "function of some underlying parameter $\\btheta$. In practice, it is often convenient \n",
    "to split the contributions of the summands by means of the majorization \n",
    "\\begin{eqnarray}\n",
    "f\\Big[\\sum_i u_i(\\btheta)\\Big] & \\le & \\sum_i \\frac{u_i(\\btheta_n)}{\\sum_j u_j(\\btheta_n)}\n",
    "f\\left[\\frac{\\sum_j u_j(\\btheta_n)}{u_i(\\btheta_n)} u_i(\\btheta)\\right] \\label{Jensen's_majorization}.\n",
    "\\end{eqnarray}\n",
    "Equality holds here when $u_i(\\btheta) = u_i(\\btheta_n)$ for all $i$. For the special case $f(u)=-\\ln u$, \n",
    "the minorization\n",
    "\\begin{eqnarray}\n",
    "\\ln \\Big[\\sum_i u_i(\\btheta)\\Big] & \\ge & \\sum_i \\frac{u_i(\\btheta_n)}{\\sum_j u_j(\\btheta_n)} \\ln u_i(\\btheta)\n",
    " +\\text{constant} \\label{log_splitting_majorization} \n",
    "\\end{eqnarray}\n",
    "relies on an irrelevant constant depending only on the current values $u_i(\\btheta_n)$. \n",
    "This minorization is handy in splitting loglikelihoods in maximum likelihood \n",
    "estimation with mixture models. \n",
    "\n",
    "The supporting hyperplane inequality \n",
    "\\begin{eqnarray}\n",
    "f(\\btheta) & \\ge & f(\\btheta_n)+ \\nabla f(\\btheta_n)^t(\\btheta-\\btheta_n)\n",
    "\\label{supporting_hyperplane_ineq}\n",
    "\\end{eqnarray}\n",
    "for a convex function is also helpful in deriving many MM algorithms.\n",
    "Here $\\nabla f(\\btheta)$ is the gradient of $f(\\btheta)$. Again the \n",
    "choice $f(u) = - \\ln u$ frequently appears.  Other inequalities\n",
    "of value include the Cauchy-Schwarz inequality, the arithmetic-geometric\n",
    "mean inequality, the triangle inequality, and the information inequality\n",
    "\\citep{steele2004cauchy}.\n",
    "\n",
    "**TODO**: plot one of later examples here to illustration minorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to summarize the riches of `Julia` in a single section of\n",
    "a short paper. The minimal guidance offered here merely \n",
    "introduces `Julia` and its lovely syntax. Once you install `Julia`, which is a trivial\n",
    "exercise in web downloading, implementing our maximum likelihood examples\n",
    "is easily accomplished by writing a short function. \n",
    "\n",
    "We illustrate some rudiments of `Julia` \n",
    "programming by implementing functions for evaluating the loglikelihood of \n",
    "Yule-Simon distribution, the first example considered in the next section. Yule-Simon \n",
    "is a discrete distribution on positive integers with probability density function (pdf)\n",
    "\\begin{eqnarray*}\n",
    "f(x \\mid \\rho) & = &  \\rho \\text{B}(x,\\rho+1)\n",
    "\\end{eqnarray*} \n",
    "on the positive integers, where \n",
    "$\\text{B}(\\alpha,\\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)$ \n",
    "is the beta function and $\\rho >0$ is a parameter. Following function \n",
    "evaluates the log-pdf of one data point `x` at parameter `ρ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yulesimon_logpdf (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yulesimon_logpdf(x::Integer, ρ::Real) = log(ρ) + lbeta(x, ρ + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code just displayed is almost transparent. Indeed, formal algorithm descriptions are nearly redundant once one understands `Julia`’s conventions, which mirror many of the conventions of `Matlab` and `R`. One of `Julia`’s other strengths is its ability to infer the types of passed variables. To achieve the best computational efficiency, this information can be directly supplied by type annotation. Thus, the declaration `x::Integer` and `ρ::Real` alerts `Julia` to the fact that the data `x` must be a subtype of the abstract type `Integer` and the parameter `ρ` must be a subtype of the abstract type `Real`. Mastery of type system allows one tap into `Julia`'s powerful multiple dispatch mechanism that dynamically dispatches functions according to the types of all arguments.\n",
    "\n",
    "`Julia` possesses a rich set of base functions. Our displayed code invokes the traditional scalar function `log` and the more exotic scalar functions `lbeta`. Scalar functions can also act on vector arguments entry by entry. For example, if `x` is a vector, then `log(x)` delivers the natural logarithm of each entry of `x`. The period sign before a vector arithmetic operator tells Julia to apply the operator entry by entry. Thus, the $i$th entry of the vector `z = x .* y` satisfies `z[i] = x[i] * y[i]`. `Julia` documentation has a more complete [list](http://docs.julialang.org/en/release-0.4/manual/arrays/?highlight=vectorize#vectorized-operators-and-functions) of vectorized functions and operators. Scalar function not in the list can be easily vectorized. For example, to evaluate pdf or logpdf at a vector of integers, we can simply use the [`comprehension`](http://docs.julialang.org/en/release-0.4/manual/arrays/#comprehensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -0.693147\n",
       " -1.79176 \n",
       " -2.48491 \n",
       " -2.99573 \n",
       " -3.4012  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[yulesimon_logpdf(x, 1.0) for x in 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or [`map`](http://docs.julialang.org/en/release-0.4/stdlib/collections/#Base.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -0.693147\n",
       " -1.79176 \n",
       " -2.48491 \n",
       " -2.99573 \n",
       " -3.4012  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(x -> yulesimon_logpdf(x, 1.0), 1:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction functions such as `sum`, `mean` and `norm` also act on vector arguments but return a scalar. `Julia` provides several convenient ways to construct reduction function. To evaluate the likelihood or log-likelihood of a random sample, we can use `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yulesimon_logpdf (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yulesimon_logpdf{T<:Integer}(x::Vector{T}, ρ::Real) = \n",
    "    sum(xi -> yulesimon_logpdf(xi, ρ), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the more generic `mapreduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.366742954792148"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yulesimon_logpdf{T<:Integer}(x::Vector{T}, ρ::Real) = \n",
    "    mapreduce(xi -> yulesimon_logpdf(xi, ρ), +, x)\n",
    "yulesimon_logpdf(collect(1:5), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type parameter `T` indicates that elements of vector `x` can be any subtype of `Integer`. Now the function definition for evaluating pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yulesimon_pdf (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yulesimon_pdf(x, ρ) = exp(yulesimon_logpdf(x, ρ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "works for both scalar and vector input `x` because of the multiple dispatch of `yulesimon_logpdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yulesimon_pdf(1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1574074074074063e-5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yulesimon_pdf(collect(1:5), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization often invokes taking derivatives of the objective function. The `ForwardDiff` package implements automatic differentiation. For example, to compute the derivative of the log-likelihood with data `x = 1, 2, ..., 5` at `ρ = 1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14999999999999858"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "x = collect(1:5)\n",
    "ForwardDiff.derivative(ρi -> yulesimon_logpdf(x, ρi), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally `Julia` extends the assignment operator to tuples. For instance, the command `(a, b) = (1.0, 2.0)` simultaneously assigns values 1.0 and 2.0 to `a` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "This section collects several examples that defy traditional calculus methods.\n",
    "For the sake of brevity, our derivations are terse. Wikipedia discusses all \n",
    "of the distributions treated here in detail. Teachers will doubtless \n",
    "want to fill in missing details for their students and winnow the \n",
    "list of examples to a more manageable size.  The list is by no means \n",
    "exhaustive.  The best students should be challenged to derive and implement \n",
    "novel MLE algorithms for other examples. Their joy in discovery might even \n",
    "propel a few to become research statisticians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yule-Simon distribution\n",
    "\n",
    "The Yule–-Simon distribution possesses the discrete density\n",
    "\\begin{eqnarray*}\n",
    "f(x \\mid \\rho) & = &  \\rho \\text{B}(x,\\rho+1)\n",
    "\\end{eqnarray*} \n",
    "on the positive integers, where \n",
    "$\\text{B}(\\alpha,\\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)$ \n",
    "is the beta function and $\\rho >0$ is a parameter. A random sample $x_1,\\ldots,x_m$ is\n",
    "summarized by its loglikelihood\n",
    "\\begin{eqnarray*}\n",
    "L(\\rho) & = & m \\ln \\rho +\\sum_{i=1}^m \\ln \\text{B}(x_i,\\rho+1).\n",
    "\\end{eqnarray*} \n",
    "The stationarity condition \n",
    "\\begin{eqnarray*}\n",
    "0 & = & \\frac{m}{\\rho} +\\sum_{i=1}^m  \\frac{d}{d \\rho} \\ln \\text{B}(x_i,\\rho+1).\n",
    "\\end{eqnarray*}\n",
    "can be simplified by writing\n",
    "\\begin{eqnarray*}\n",
    "\\frac{d}{d \\rho} \\ln \\text{B}(x_i,\\rho+1) & = & \\frac{\\frac{\\Gamma(x_i)\\frac{d}{d \\rho} \\Gamma(\\rho+1)}\n",
    "{\\Gamma(x_i+\\rho+1)}}{\\frac{\\Gamma(x_i) \\Gamma(\\rho+1)}{\\Gamma(x_i+\\rho+1)}}\n",
    "-\\frac{\\frac{\\Gamma(x_i)\\Gamma(\\rho+1)}\n",
    "{\\Gamma(x_i+\\rho+1)^2}}{\\frac{\\Gamma(x_i) \\Gamma(\\rho+1)}{\\Gamma(x_i+\\rho+1)}}\n",
    "\\frac{d}{d \\rho} \\Gamma(x_i+\\rho+1) \\\\\n",
    "& = & \\psi(\\rho+1)-\\psi(x_i+\\rho+1)\n",
    "\\end{eqnarray*}\n",
    "in terms of the digamma function $\\psi(t) = \\frac{d}{dt} \\ln \\Gamma(t)$.  The stationarity equation\n",
    "\\begin{eqnarray*}\n",
    "0 & = & \\frac{m}{\\rho} +\\sum_{i=1}^m [\\psi(\\rho+1)-\\psi(x_i+\\rho+1)] \n",
    "\\end{eqnarray*}\n",
    "is readily solved by Julia. Alternatively, one can derive an MM algorithm by capitalizing\n",
    "on the log-convexity of the beta function. We invite the reader to fill in the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finding MLE boils down to finding the zero of derivative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_yule_simon (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_yule_simon{T<:Integer}(x::Vector{T})\n",
    "    return fzero(ρ -> ForwardDiff.derivative(ρi -> yulesimon_logpdf(x, ρi), ρ), 1e-6, 10.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter:\n",
      "ρ = [3.8422383759828493]\n",
      "MoM estimator:\n",
      "ρ̂ = 1.0196278558530265\n",
      "MLE by MM:\n",
      "ρ̂ = 0.25327294285703383\n"
     ]
    }
   ],
   "source": [
    "using Distributions, Roots\n",
    "srand(123)\n",
    "(n, p) = (1000, 1)\n",
    "ρ = 5.0rand(p)\n",
    "println(\"True parameter:\")\n",
    "println(\"ρ = \", ρ)\n",
    "# TODO: generate from Yule-Simon\n",
    "x = rand(DiscreteUniform(1, 100), n)\n",
    "println(\"MoM estimator:\")\n",
    "println(\"ρ̂ = \", mean(x) / (mean(x) - 1))\n",
    "println(\"MLE by MM:\")\n",
    "ρ = mle_yule_simon(x)\n",
    "println(\"ρ̂ = \", ρ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gompertz Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gompertz distribution is a continuous density on $[0,\\infty)$ with density\n",
    "\\begin{eqnarray*}\n",
    "f(x \\mid \\eta, \\beta) = \\beta \\eta e^{\\beta x}e^{\\eta}e^{-\\eta e^{\\beta x}} .\n",
    "\\end{eqnarray*}\n",
    "Here $\\beta > 0$ is the scale parameter, and $\\eta > 0$ is the shape parameter.\n",
    "The loglikelihood\n",
    "\\begin{eqnarray*}\n",
    "m \\ln \\beta +m  \\ln \\eta + m \\eta+ \\beta \\sum_{i=1}^m x_i  \n",
    "- \\eta \\sum_{i=1}^m  e^{\\beta x_i}\n",
    "\\end{eqnarray*}\n",
    "of a random sample $x_1,\\ldots,x_m$ can be easily maximized with respect to\n",
    "$\\eta$. Inserting the optimal value\n",
    "\\begin{eqnarray*}\n",
    "    \\eta = \\left(\\frac{1}{m}\\sum_{i=1}^m e^{\\beta x_i} -1 \\right)^{-1}\n",
    "\\end{eqnarray*}\n",
    "into the loglikelihood yields the profile loglikelihood\n",
    "\\begin{eqnarray*}\n",
    "m \\ln \\beta - m \\ln \\Big(\\frac{1}{m}\\sum_{i=1}^m e^{\\beta x_i} -1\\Big)+ m \\beta \\bar{x}+\n",
    "\\text{constant}\n",
    "\\end{eqnarray*}\n",
    "in terms of the sample average $\\bar{x}$. The stationarity condition\n",
    "\\begin{eqnarray*}\n",
    "0 & = & m \\left[\\frac{1}{\\beta}- \\frac{\\frac{1}{m}\\sum_{i=1}^m \n",
    "e^{\\beta x_i}x_i}{\\frac{1}{m}\\sum_{i=1}^m e^{\\beta x_i} -1} +\\bar{x}\\right] \n",
    "\\end{eqnarray*}\n",
    "succumbs to root finding by Julia. One can show that this equation possesses\n",
    "a unique positive root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_gompertz (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_gompertz{T<:Real}(x::Vector{T})\n",
    "    m, avg = length(x), mean(x)\n",
    "    f(r) = 1 / r - sum(xi -> exp(r * xi) * xi, x) / \n",
    "        (sum(xi -> exp(r * xi), x) - m) + avg\n",
    "    β = fzero(f, 1e-6, 10.0)\n",
    "    η = 1 / (sum(xi -> exp(β * xi), x) / m - 1)\n",
    "    return (β, η)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE:\n",
      "β̂ = 1.2642562810214624, η̂ = 0.2930748117838888\n"
     ]
    }
   ],
   "source": [
    "# TODO generate from Gompertz\n",
    "x = 2rand(n)\n",
    "(β, η) = mle_gompertz(x)\n",
    "println(\"MLE:\")\n",
    "println(\"β̂ = \", β,\", η̂ = \", η)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibull Distribution\n",
    "\n",
    "Consider a random sample $x_1,\\ldots,x_m$ from a two-parameter Weibull density\n",
    "\\begin{eqnarray*}\n",
    "f(x) & = & \\frac{\\kappa}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{\\kappa-1}\n",
    "e^{-(x/\\lambda)^{\\kappa}}\n",
    "\\end{eqnarray*}  \n",
    "over the interval $(0,\\infty)$. One can check that the maximum of the loglikelihood \n",
    "with respect to $\\lambda >0$ for $\\kappa>0$ fixed satisfies the equation\n",
    "$\\lambda^\\kappa = {\\frac{1}{m}\\sum_{i=1}^m x_i^\\kappa}$ \\citep{balakrishnan2008maximum}.\n",
    "Substituting this $\\lambda$ into the ordinary loglikelihood\n",
    "yields the profile loglikelihood \n",
    "\\begin{eqnarray*}\n",
    " m \\ln \\kappa - m\\ln \\left( \\sum_{i=1}^m e^{\\kappa \\ln x_i}\\right)+\n",
    " (\\kappa-1) \\sum_{i=1}^m\\ln x_i +\\text{constant}.\n",
    " \\end{eqnarray*}\n",
    "Since the second term of the profile loglikelihood is concave in $\\kappa$\n",
    "\\citep[Example 6.3.10]{lange2013optimization}\n",
    "one can apply the majorization (\\ref{Jensen's_majorization}). This leads to the MM algorithm \n",
    "\\begin{eqnarray*}\n",
    "\\frac{1}{\\kappa_{n+1}} & = & \\frac{\\sum_{i=1}^m x_i^{\\kappa_n}\\ln x_i}{\\sum_{i=1}^m x_i^{\\kappa_n}}\n",
    "-\\frac{1}{m} \\sum_{i=1}^m \\ln x_i.\n",
    "\\end{eqnarray*} \n",
    "Given a reasonable amount of data, the algorithm converges quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_weibull (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_weibull{T<:Real}(x::Vector{T})\n",
    "    (κ, old_κ) = (1.0, 1.0)\n",
    "    avglog = mean(log(x))\n",
    "    for iteration = 1:500\n",
    "        a = sum(xi -> xi^κ * log(xi), x)\n",
    "        b = sum(xi -> xi^κ, x)\n",
    "        κ = 1 / (a / b - avglog)\n",
    "        if abs(κ - old_κ) < 1e-6\n",
    "            break \n",
    "        else\n",
    "            old_κ = κ\n",
    "        end\n",
    "    end\n",
    "    λ = sum(xi -> xi^κ, x) / length(x)\n",
    "    return (κ, λ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter:\n",
      "κ =0.9565905338307035, λ = 1.5940072852898513\n",
      "MLE:\n",
      "κ̂ = 0.9482053262447676 λ̂ = 1.5145321220672565\n"
     ]
    }
   ],
   "source": [
    "(n, p) = (1000, 2)\n",
    "(κ, λ) = 5rand(p)\n",
    "println(\"True parameter:\")\n",
    "println(\"κ =\", κ, \", λ = \", λ)\n",
    "x = rand(Weibull(κ, λ), 1000)\n",
    "println(\"MLE:\")\n",
    "(κ, λ) = mle_weibull(x)\n",
    "println(\"κ̂ = \", κ,\" λ̂ = \", λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rice Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plane, pick a point $\\bp$ at a distance $\\nu$ from the origin and generate\n",
    "a Gaussian random vector $\\by$ with mean $\\bf 0$ and covariance matrix $\\sigma^2 \\bI$.\n",
    "The distance of the point $\\by+\\bp$ from the origin follows Rice's distribution. \n",
    "One can show the that corresponding density is\n",
    "\\begin{eqnarray*}\n",
    "f(x\\mid\\nu,\\sigma^2) & = & \\frac{x}{\\sigma^2}e^{-\\frac{x^2+\\nu^2} {2\\sigma^2}}\n",
    "I_0\\left(\\frac{x\\nu}{\\sigma^2}\\right),\n",
    "\\end{eqnarray*}\n",
    "where $I_0(z)$ is the modified Bessel function of the first kind \n",
    "\\begin{eqnarray*}\n",
    "I_n(z) & = & \\frac{1}{\\pi} \\int_0^\\pi e^{z \\cos \\theta} \\cos( n \\theta) d\\theta \n",
    "\\end{eqnarray*}\n",
    "of degree $n=0$. One can argue that $I_0(z)$ is log-convex with derivative\n",
    "$I_1(z)$. (Log-convexity is preserved by integration, and the integrand\n",
    "$e^{z \\cos \\theta}$ of $I_0(z)$ is log-convex \\citep{lange2013optimization}.) \n",
    "According to the supporting hyperplane inequality (\\ref{supporting_hyperplane_ineq}),\n",
    "the loglikelihood \n",
    "\\begin{eqnarray*}\n",
    "-m \\ln \\sigma^2 + \\sum_{i=1}^m \\ln x_i - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^m x_i^2\n",
    "- \\frac{m}{2 \\sigma^2} \\nu^2\n",
    "+ \\sum_{i=1}^m \\ln I_0\\left(\\frac{x_i\\nu}{\\sigma^2}\\right)\n",
    "\\end{eqnarray*}\n",
    "of a sample $x_1,\\ldots,x_m$ is minorized by the surrogate function\n",
    "\\begin{eqnarray*}\n",
    "g(\\nu,\\sigma^2 \\mid \\nu_n, \\sigma_n^2) & = & \n",
    "-m \\ln \\sigma^2 + \\sum_{i=1}^m \\ln x_i - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^m x_i^2\n",
    "- \\frac{m}{2\\sigma^2}\\nu^2 \n",
    "+ \\sum_{i=1}^m w_{ni} \\frac{x_i\\nu}{\\sigma^2} \\\\\n",
    "w_{ni} & = & I_1\\left(\\frac{x_i \\nu_n}{\\sigma_n^2} \\right)\n",
    "\\end{eqnarray*}\n",
    "up to an irrelevant constant. The corresponding stationarity equations \n",
    "can be solved for the MM updates\n",
    "\\begin{eqnarray*}\n",
    "\\nu_{n+1} & = & \\frac{1}{m} \\sum_{i=1}^m w_{ni} x_i \\\\\n",
    "\\sigma_{n+1}^2 & = & \\frac{1}{m} \\left[ \\frac{1}{2}\\sum_{i=1}^m x_i^2\n",
    "+ \\frac{m}{2} \\nu_{n+1}^2-\\nu_{n+1} \\sum_{i=1}^m w_{ni}x_i  \\right] \\\\\n",
    "& = & \\frac{1}{2} \\left[ \\frac{1}{m}\\sum_{i=1}^m x_i^2\n",
    "- \\nu_{n+1}^2 \\right].\n",
    "\\end{eqnarray*}\n",
    "Given that Bessel functions are part of the base code of Julia,\n",
    "these updates are easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_rice (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_rice{T<:Real}(x::Vector{T})\n",
    "    avgsq = sumabs2(x) / length(x)\n",
    "    (ν, old_ν) = (one(T), one(T))\n",
    "    (σ2, old_σ2) = (one(T), one(T))\n",
    "    for iteration = 1:500\n",
    "        r  = ν / σ2\n",
    "        ν  = sum(xi -> besseli(1, xi * r), x) / length(x)\n",
    "        σ2 = (avgsq - ν^2) / 2\n",
    "        if abs(ν - old_ν) + abs(σ2 - old_σ2) < 1e-6\n",
    "            break\n",
    "        else\n",
    "            (old_ν, old_σ2) = (ν, σ2)\n",
    "        end\n",
    "    end\n",
    "    return (ν, σ2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter value:\n",
      "ν = 1.5625603209510963, σ2 = 0.45538165001822395\n",
      "MLE by MM:\n",
      "ν̂ = -1.0314877589577705e-6, σ̂2 = 1.700647014215347\n"
     ]
    }
   ],
   "source": [
    "(n, p) = (1000, 2)\n",
    "(ν, σ2) = 5.0 * rand(p)\n",
    "println(\"True parameter value:\")\n",
    "println(\"ν = \", ν, \", σ2 = \", σ2)\n",
    "srand(123)\n",
    "# generate a random sample\n",
    "x = zeros(n)\n",
    "for i = 1:n\n",
    "    θ = 360rand()\n",
    "    x[i] = norm(√σ2 * randn(2) + [ν * cos(θ), ν * sin(θ)])\n",
    "end\n",
    "println(\"MLE by MM:\")\n",
    "(ν, σ2) = mle_rice(x)\n",
    "println(\"ν̂ = \", ν, \", σ̂2 = \", σ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution \\citep{kingman1992poisson} is used to model \n",
    "random proportions. It has probability density\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\Gamma(\\sum_{i=1}^r \\theta_i)}\n",
    "{\\prod_{i=1}^r \\Gamma(\\theta_i)} \\prod_{i=1}^r x_i^{\\theta_i - 1}\n",
    "\\label{dirichlet}\n",
    "\\end{eqnarray}\n",
    "on the unit simplex \n",
    "$\\{\\bx = (x_1,\\ldots,x_r)^t : x_1 > 0,\\ldots,x_r > 0 , \\sum_{i=1}^r x_i = 1 \\}$\n",
    "endowed with the uniform measure.  The beta distribution is the\n",
    "special case $r=2$. \n",
    "\n",
    "If $\\bx_{1},\\ldots,\\bx_{m}$ are randomly sampled vectors from the \n",
    "Dirichlet distribution, then their loglikelihood is\n",
    "\\begin{eqnarray*}\n",
    "L(\\btheta) & = & m \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_i\\Big)\n",
    "-m \\sum_{i=1}^r \\ln \\Gamma(\\theta_i) + \\sum_{j=1}^m\n",
    "\\sum_{i=1}^r (\\theta_i - 1) \\ln x_{ji}.\n",
    "\\end{eqnarray*}\n",
    "Except for the first term on the right, the parameters are separated.  \n",
    "Fortunately, the function $\\ln \\Gamma(t)$ is convex.  Its derivative, the\n",
    "digamma function, is denoted by $\\psi(t)$. The minorization\n",
    "\\begin{eqnarray*}\n",
    "\\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_i\\Big)\n",
    "& \\ge & \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_{ni} \\Big)\n",
    "+ \\psi\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) \\sum_{i=1}^r (\\theta_i-\\theta_{ni})\n",
    "\\end{eqnarray*}\n",
    "is a special case of the minorization (\\ref{supporting_hyperplane_ineq}) and\n",
    "generates the surrogate function\n",
    "\\begin{eqnarray*}\n",
    "g(\\btheta \\mid \\btheta_{n}) & = & m \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big)\n",
    "+ m \\psi\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) \\sum_{i=1}^r (\\theta_i-\\theta_{ni})\\\\\n",
    "&   & -\\, m \\sum_{i=1}^r \\ln \\Gamma(\\theta_i) + \\sum_{j=1}^m\n",
    "\\sum_{i=1}^r (\\theta_i - 1) \\ln x_{ji}.\n",
    "\\end{eqnarray*}\n",
    "Owing to the presence of the terms $\\ln \\Gamma(\\theta_i)$, the\n",
    "maximization step is analytically intractable.  However, Julia can readily solve\n",
    "the stationarity equation for each $\\theta_i$.  The current\n",
    "treatment of the MM algorithm should be compared to\n",
    "the earlier treatment in \\citep{lange1995gradient} and the\n",
    "more traditional treatment in \\citep{narayanan1991algorithm}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_dirichlet (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_dirichlet{T<:Real}(x::Matrix{T})\n",
    "    p = size(x, 1)\n",
    "    avglog = sum(log, x, 2) / size(x, 2)\n",
    "    (θ, old_θ) = (ones(T, p), ones(T, p))\n",
    "    for iteration = 1:500\n",
    "        c = digamma(sum(θ))\n",
    "        for i = 1:p\n",
    "            θ[i] = fzero(r -> avglog[i] + c - digamma(r), 1e-6, Inf)\n",
    "        end\n",
    "        if norm(θ - old_θ) < 1e-6 \n",
    "            break\n",
    "        else\n",
    "            copy!(old_θ, θ)\n",
    "        end \n",
    "    end\n",
    "    return θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter value:\n",
      "θ = [2.8389216644390336,3.1590692068357074,2.5000118372115097]\n",
      "MLE by Distribution package:\n",
      "θ̂ = [2.894891772116383,3.2172650428961727,2.6016473656788173]\n",
      "MLE by MM:\n",
      "θ̂ = [2.8948872423415035,3.2172599186621604,2.601643375029334]\n"
     ]
    }
   ],
   "source": [
    "(n, p) = (1000, 3)\n",
    "θ = 5.0 * rand(p)\n",
    "println(\"True parameter value:\")\n",
    "println(\"θ = \", θ)\n",
    "x = rand(Dirichlet(θ), n)\n",
    "println(\"MLE by Distribution package:\")\n",
    "println(\"θ̂ = \", fit_mle(Dirichlet, x).alpha)\n",
    "println(\"MLE by MM:\")\n",
    "θ = mle_dirichlet(x)\n",
    "println(\"θ̂ = \", θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative-binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative binomial distribution has density $\\binom{x+r-1}{x}p^r(1-p)^x$\n",
    "over the nonnegative integers $x$. For a random sample $x_1,\\ldots,x_m$ with\n",
    "$r$ fixed, one can easily derive the maximum likelihood estimate \n",
    "\\begin{eqnarray}\n",
    "\\hat{p} = \\frac{r}{r+ \\bar{x}} \\label{p_update}\n",
    "\\end{eqnarray}\n",
    "involving the sample mean $\\bar{x}$. If $r$ is considered a parameter, \n",
    "then the method of moments estimators\n",
    "\\begin{eqnarray*}\n",
    "\\hat{p} = \\frac{\\bar{x}}{s^2} \\quad \\text{and} \\quad\n",
    "\\hat{r} = \\frac{\\bar{x}^2}{s^2 -\\bar{x}}\n",
    "\\end{eqnarray*}\n",
    "apply, where $s^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i-\\overline{x})^2$ is the\n",
    "sample variance. The estimator of $r$ is undefined when $\\bar{x} \\ge s^2$.\n",
    "The gamma functions can be eliminated from the loglikelihood by noting\n",
    "that\n",
    "\\begin{eqnarray*}\n",
    "L(p,r) & = & \\sum_{i=1}^m \\left[\\ln \\binom{x_i+r-1}{x_i} + r \\ln p + x_i \\ln (1-p)\\right] \\\\\n",
    "& = & \\sum_{i=1}^m \\Big[\\sum_{j=0}^{x_i-1} \\ln(r+j) -\\ln x_i!  + x_i \\ln (1-p)\\Big]  +mr \\ln p\n",
    "\\end{eqnarray*}\n",
    "The minorization (\\ref{log_splitting_majorization}) now implies\n",
    "$\\ln(r+j) \\ge  \\frac{r_n}{r_n+j} \\ln r +c_{nj}$ for some irrelevant constant $c_{nj}$.\n",
    "Substituting this minorization for $\\ln(r+j)$ in $L(p,r)$ gives\n",
    "a surrogate with maximum point\n",
    "\\begin{eqnarray}\n",
    "r_{n+1} & = & - \\frac{\\sum_{i=1}^m \\sum_{j=0}^{x_i-1} \\frac{r_n}{r_n+j}}{m \\ln p_n}.\n",
    "\\label{r_update}\n",
    "\\end{eqnarray}\n",
    "Although alternating the updates (\\ref{p_update}) and (\\ref{r_update}) is not\n",
    "quite block ascent, it is guaranteed to increase the loglikelihood $L(p,r)$\n",
    "and has proven quite effective in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_negative_binomial (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_negative_binomial{T<:Integer}(x::Vector{T})\n",
    "    m = length(x)\n",
    "    (p, r) = (0.5, 1.0)\n",
    "    (old_p, old_r) = (p, r)\n",
    "    avg = mean(x)\n",
    "    for iteration = 1:500\n",
    "        s = 0.0\n",
    "        for i = 1:m\n",
    "            for j = 0:(x[i] - 1)\n",
    "                s = s + r / (r + j)\n",
    "            end\n",
    "        end\n",
    "        r = - s / (m * log(p))\n",
    "        p = r / (r + avg)\n",
    "        if abs(p - old_p) + abs(r - old_r) < 1e-6\n",
    "            break \n",
    "        else\n",
    "            (old_p, old_r) = (p, r) \n",
    "        end\n",
    "    end\n",
    "    return (p, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter value:\n",
      "p = 0.25, r = 5.0\n",
      "MLE by MM:\n",
      "p̂ = 0.2557868696071902, r̂ = 5.062029762342788\n"
     ]
    }
   ],
   "source": [
    "srand(123)\n",
    "(n, r, p) = (1000, 5.0, 0.25)\n",
    "println(\"True parameter value:\")\n",
    "println(\"p = \", p, \", r = \", r)\n",
    "x = rand(NegativeBinomial(r, p), n)\n",
    "println(\"MLE by MM:\")\n",
    "(p, r) = mle_negative_binomial(x)\n",
    "println(\"p̂ = \", p, \", r̂ = \", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_1,\\ldots,x_m$ be a random sample from the gamma density\n",
    "\\begin{eqnarray*}\n",
    "f(x) & = & \\Gamma(\\alpha)^{-1} \\beta^{\\alpha} x^{\\alpha-1} e^{-\\beta x}\n",
    "\\end{eqnarray*}\n",
    "on $(0,\\infty)$. It is straightforward to derive the method of moments\n",
    "estimators\n",
    "\\begin{eqnarray*}\n",
    "\\hat{\\alpha} \\;\\; = \\;\\; \\frac{\\overline{x}^2}{s^2} \\quad \\text{and} \\quad \n",
    "\\hat{\\beta} & = & \\frac{\\overline{x}}{s^2},\n",
    "\\end{eqnarray*}\n",
    "where $\\overline{x} = \\frac{1}{m} \\sum_{i=1}^m x_i$ and \n",
    "$s^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i-\\overline{x})^2$ are the sample mean \n",
    "and variance, respectively. These are not necessarily the best explicit estimators\n",
    "of the two parameters.  Setting the score function equal to \n",
    "${\\bf 0}$ implies that $\\beta = \\alpha/\\overline{x}$ is a stationary\n",
    "point of the loglikelihood $L(\\alpha,\\beta)$ of the sample for \n",
    "$\\alpha$ fixed. In fact, $\\beta = \\alpha/\\overline{x}$ furnishes the maximum. \n",
    "Substituting this value of $\\beta$ in the loglikelihood reduces \n",
    "maximum likelihood estimation to optimization of the profile loglikelihood\n",
    "\\begin{eqnarray*}\n",
    "L(\\alpha) & = & m \\alpha \\ln \\alpha- m \\alpha \\ln \\overline{x} - m \\ln \\Gamma(\\alpha) \n",
    "+ m(\\alpha-1) \\overline{\\ln x}-m\\alpha.\n",
    "\\end{eqnarray*}\n",
    "Here $\\overline{\\ln x} = \\frac{1}{m} \\sum_{i=1}^m \\ln x_i$. The\n",
    "stationarity equation\n",
    "\\begin{eqnarray*}\n",
    "0 & = & m \\left( \\ln \\alpha -  \\ln \\overline{x} -  \\psi(\\alpha) \n",
    "+  \\overline{\\ln x} \\, \\right)\n",
    "\\end{eqnarray*}\n",
    "is ripe for solution by Julia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_gamma (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_gamma{T<:Real}(x::Vector{T})\n",
    "    avg = mean(x)\n",
    "    d = log(avg) - sum(log, x) / length(x)\n",
    "    α = fzero(r -> log(r) - digamma(r) - d, 1e-6, Inf)\n",
    "    β = α / avg\n",
    "    return (α, β)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can attempt to approximate the maximum likelihood\n",
    "solution. There are two nasty terms in $L(\\alpha)$. One is $\\alpha \\ln \\alpha$, \n",
    "and the other is $\\ln \\Gamma(\\alpha)$. We can eliminate both by appealing \n",
    "to a version of Stirling's formula.  Ordinarily Stirling's formula\n",
    "is only applied for large factorials. This limitation is inconsistent with \n",
    "small $\\alpha$. However, Gosper's \\citep{gosper1978decision} version \n",
    "of Stirling's formula is accurate for all arguments. This little-known \n",
    "version of Stirling's formula says that\n",
    "\\begin{eqnarray*}\n",
    "\\Gamma(\\alpha+1) & \\approx & \\sqrt{(\\alpha+1/6)2 \\pi} \\, \\alpha^\\alpha e^{-\\alpha}.\n",
    "\\end{eqnarray*}\n",
    "Given that $\\Gamma(\\alpha) = \\Gamma(\\alpha+1)/\\alpha$, one can show that \n",
    "substitution of Gosper's formula in the profile loglikelihood leads to the \n",
    "approximate maximum likelihood estimate\n",
    "\\begin{eqnarray*}\n",
    "\\hat{\\alpha} & = & \\frac{3-d+ \\sqrt{(3-d)^2+24d}}{12d} ,\n",
    "\\end{eqnarray*}\n",
    "where $d = \\ln \\overline{x}-\\overline{\\ln x}$ \\citep{choi1969maximum}.  To force\n",
    "the estimate of $\\alpha$ to be positive, one needs to take the larger root of\n",
    "the defining quadratic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stirling_gamma (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function stirling_gamma{T<:Real}(x::Vector{T})\n",
    "    avg = mean(x)\n",
    "    d = log(avg) - sum(log, x) / length(x)\n",
    "    α = (3 - d + sqrt((3 - d)^2 + 24d)) / (12d)\n",
    "    β = α / avg\n",
    "    return (α, β)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter values:\n",
      "α = 3.8422383759828493, β = 4.7025750035759355\n",
      "Estimator by Stirling approximation:\n",
      "α̂ = 3.641077654152312, β̂ = 4.867151912108691\n",
      "Estimator by MLE:\n",
      "α̂ = 3.646663250650827, β̂ = 4.859696892325012\n",
      "Estimator by MLE (Distribution package):\n",
      "Distributions.Gamma{Float64}(α=3.646663250650781, θ=4.8596968923250845)\n"
     ]
    }
   ],
   "source": [
    "srand(123)\n",
    "(n, p) = (1000, 2)\n",
    "(α, β) = 5.0 * rand(p)\n",
    "println(\"True parameter values:\")\n",
    "println(\"α = \", α, \", β = \", β)\n",
    "x = rand(Gamma(α, β), n)\n",
    "println(\"Estimator by Stirling approximation:\")\n",
    "(α, β) = stirling_gamma(x)\n",
    "println(\"α̂ = \", α, \", β̂ = \", 1.0 / β)\n",
    "println(\"Estimator by MLE:\")\n",
    "(α, β) = mle_gamma(x)\n",
    "println(\"α̂ = \", α, \", β̂ = \", 1.0 / β)\n",
    "println(\"Estimator by MLE (Distribution package):\")\n",
    "println(fit_mle(Gamma, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples presented here showcase in miniature some of the optimization\n",
    "techniques encountered in big data analysis. Students should not be left\n",
    "to fend for themselves in difficult estimation problems. With patience,\n",
    "many of the MLE problems that vexed previous\n",
    "generations can be solved with little programming effort. It is merely a matter of\n",
    "putting the right tools in the hands of statisticians. Julia is one such\n",
    "tool. Its rich mathematical libraries and simple syntax make coding easy.\n",
    "Regardless of the programming language, themes such as block ascent, profile\n",
    "likelihoods, and the MM principle figure prominently in high-dimensional estimation.\n",
    "The sooner these lessons are absorbed, the sooner students can embrace estimation\n",
    "in big data with confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  },
  "latex_envs": {
   "bibliofile": "biblo.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
